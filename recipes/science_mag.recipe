import string, re

from collections import OrderedDict

from calibre.web.feeds.recipes import BasicNewsRecipe
from calibre.ebooks.BeautifulSoup import BeautifulSoup

base_url = "http://science.sciencemag.org"
class ScienceMag(BasicNewsRecipe):

    title       = 'Science Magazine'
    needs_subscription = True
    publication_type = "magazine"
    __author__  = 'Noah Pryor'
    oldest_article = 30
    description = "Downloads the latest issue of ScienceMag"
    needs_subscription = True
    no_stylesheets = True
    # auto_cleanup = True
    keep_only_tags = [dict(id=['content-block-markup'])]
    remove_tags = [
        dict(name='div', attrs={'class': ['attrib', 'fig-caption attrib', 'article__expandable-area', 'contributors', 'highwire-figure-links']}),
        dict(name='a', attrs={'class': ['highwire-figure-links']}),
        dict(name="a", target="_blank"),
        dict(name="ul", attrs={'class': ['highwire-figure-links']}),
        dict(name="q")
        ]
    #
    # remove_tags_after  = dict(id='article')


    def get_browser(self):
        br = BasicNewsRecipe.get_browser(self)
        if self.username is not None and self.password is not None:
            br.open('http://science.sciencemag.org/user/login?destination=/front')
            print "Logging in"
            # find the aas login link
            url_regex=re.compile(".*signin\.aaas\.org.*")
            login_link = br.find_link(url_regex=re.compile(".*signin\.aaas\.org.*"))
            br.follow_link(login_link)
            print("Logging in")
            br.select_form(name='loginForm')
            br['loginForm:username']   = self.username
            br['loginForm:password'] = self.password
            br.submit()

            print("Redirect to " + br.geturl() + " after login")

        return br

    def parse_index(self):
        soup = self.index_to_soup('http://science.sciencemag.org/content/current')
        feeds = []
        articles = []

        issue_name = soup.find("meta", {"name": "DC.Title"})["content"]
        issue_date = soup.find("meta", {"name": "DC.Date"})["content"]
        self.cover_url = soup.find("meta", {"name": "citation_image"})["content"]
        print("Issue: " + issue_name)
        self.title = issue_name + " " + issue_date
        # Iterate through the sections of the table of contents
        # import code; code.interact(local=dict(globals(), **locals()))
        table_of_contents = soup.find("ul", "issue-toc item-list")
        contents = table_of_contents.find("li", "issue-toc-section issue-toc-section-contents")
        sections = contents.find("ul", "toc-section item-list").contents

        print(str(len(sections)) + " sections found")
        for section in sections:
            section_name = self.tag_to_string(section.find("h2"))
            if (section_name == "Reports") or (section_name == "Research Articles"):
                print("skipping section " + section_name)
                continue

            toc_items = section.findAll("div", {"class": "toc-citation"})
            # print(section_name + " - articles found: " + str(len(toc_items)))

            articles = []


            for item in toc_items:
                headline_link = item.find("a", {"class": "highwire-cite-linked-title"})
                title = self.tag_to_string(headline_link)
                full_text_path = base_url + headline_link['href'] + ".full"

                # Doesn't always exist
                description_element = item.find("div", {"class": "highwire-cite-snippet"})
                description = ""
                if description_element:
                    description = self.tag_to_string(description_element)

                article = {'title': title, 'date': issue_date, 'url': full_text_path, 'description': description }

                articles.append(article)

            # print(title)

            feeds.append((section_name, articles))
        print feeds
        return feeds